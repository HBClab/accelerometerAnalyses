ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)
# setwd sets working directory for your programs and data files
setwd("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/Post")
getwd()    # show wd to confirm
# read csv file with filenames, date(s) & time(s) on/off
st= read.csv("specs.csv", header=TRUE, sep=',', stringsAsFactors= FALSE)
#st$start <- strptime(st$start,format="%d-%b-%Y %I:%M %p")
#st$stop  <- strptime(st$stop, format="%d-%b-%Y %I:%M %p")
print(st)
str(st)
# these are the models from the paper
load("mods.RData")
# these are functions to estimate features, load data, etclis
source("fuctions.for.models.60hz.R")
win.width <- 60     # width of "windows" to analyze in seconds aka EPOCH
#start of function to run in batch mode - similar to Macro in SAS
main <- function(id, file.1, start, stop, outfile) {
# convert strings to datetime format
start <- strptime(start,format="%d-%b-%Y %I:%M %p")
stop  <- strptime(stop, format="%d-%b-%Y %I:%M %p")
str(id)
str(file.1)
str(start)
str(stop)
str(outfile)
# get start time & start date from csv file header section
hertz = 60
head.data <- readLines(paste(file.1),n=10)
start.time <- head.data[3]
start.time <- (strsplit(start.time,split=" ")[[1]][3])
start.date <- head.data[4]
start.date <- (strsplit(start.date,split=" ")[[1]][3])
start.time <- as.POSIXlt(strptime(paste(start.date,start.time),"%m/%d/%Y %H:%M:%S"))
n.to.skip = as.numeric(difftime(start,start.time,units="secs")*hertz)
n.to.read = as.numeric(difftime(stop,start,units="secs")*hertz)
ag.data <- fread(file.1,skip=10+n.to.skip,sep=",",header=F,colClasses=c("numeric","numeric","numeric"),showProgress=FALSE,nrows=n.to.read)
n <- dim(ag.data)[1]
# data.frame adds a new column with times
ag.data <- data.frame(time = start + (0:(n-1))/hertz,ag.data)
n <- dim(ag.data)[1]
mins <- ceiling(n/(60*win.width))
# compute statistics (features)
ag.data$min <- rep(1:mins,each=win.width*60)[1:n]
ag.data$vm <- sqrt(ag.data$V1^2+ag.data$V2^2+ag.data$V3^2)
ag.data$v.ang <- 90*(asin(ag.data$V1/ag.data$vm)/(pi/2))
ag.data.sum <- data.frame(mean.vm=tapply(ag.data$vm,ag.data$min,mean,na.rm=T),
sd.vm=tapply(ag.data$vm,ag.data$min,sd,na.rm=T),
mean.ang=tapply(ag.data$v.ang,ag.data$min,mean,na.rm=T),
sd.ang=tapply(ag.data$v.ang,ag.data$min,sd,na.rm=T),
p625=tapply(ag.data$vm,ag.data$min,pow.625),
dfreq=tapply(ag.data$vm,ag.data$min,dom.freq),
ratio.df=tapply(ag.data$vm,ag.data$min,frac.pow.dom.freq))
# Next line can be slow... (there is a faster library, but I haven't implemented it yet.)
ag.data.sum$start.time <- as.POSIXlt(tapply(ag.data$time,ag.data$min,min,na.rm=T),origin="1970-01-01 00:00.00 UTC")
# apply the models (estimates are for each 15 second epoch)
# MET estimates by random forest
ag.data.sum$METs.rf <- predict(rf.met.model,newdata=ag.data.sum)
ag.data.sum$METs.rf[ag.data.sum$sd.vm==0] <- 1
# MET estimates by linear regression
ag.data.sum$METs.lm <- predict(lm.met.model,newdata=ag.data.sum)
ag.data.sum$METs.lm[ag.data.sum$sd.vm==0] <- 1
# MET level estimates (rf and tree)
ag.data.sum$MET.lev.rf <- predict(rf.met.level.model,newdata=ag.data.sum)
ag.data.sum$MET.lev.tr <- predict(tr.met.level.model,newdata=ag.data.sum,type="class")
# sedentary or not estimates (rf and tree)
ag.data.sum$sed.rf <- predict(rf.sed.model,newdata=ag.data.sum)
ag.data.sum$sed.tr <- predict(tr.sed.model,newdata=ag.data.sum,type="class")
# locomotion or not estimates (rf and tree)
ag.data.sum$loc.rf <- predict(rf.loc.model,newdata=ag.data.sum)
ag.data.sum$loc.tr <- predict(tr.loc.model,newdata=ag.data.sum,type="class")
# end of calculations
print("Done with calculations")
# export entire results vector (array) to .csv text file
write.csv(ag.data.sum,outfile, row.names=FALSE, quote=FALSE)
# end of main function
}
# call function, args are by position, call by value in R
mapply(main, st$Id, st$file.1, st$start, st$stop, st$outfile)
# end of program-- you should now see .out files for each day of accelerometer data for subject
# load needed R libraries  (make sure GGIR and zoo are installed)
library(GGIR)
library(zoo)
#g.shell.GGIR calls g.part1 - g.part4
#see manual for clarification on optional input arguments
##Usage  g.shell.GGIR(mode=c(1,2),datadir=c(),outputdir=c(),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
#manually change the folder to the correct subject number (not sure how to make loops in R)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/Post/767"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results/Post"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
#example: run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Users/rachclark/Desktop/Accelerometer/775"),outputdir=c("/Volumes/vosslablss/Accelerometer/ActiGraph/Analysis/HildebrandAnalysis"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
# batch mode for left/right wrist @60hz (originally created by Rick Paulos)
# first modified by Rachel Clark, 12/14/15
# incorporated into R notebook fall 2017
# comments start with # sign
# clear work space
rm(list=ls())
# load needed R libraries  (previously installed)
library(tree)
library(randomForest)
library(data.table)
# Show installed libraries
ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)
# setwd sets working directory for your programs and data files
setwd("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data")
getwd()    # show wd to confirm
# read csv file with filenames, date(s) & time(s) on/off
st= read.csv("specs.csv", header=TRUE, sep=',', stringsAsFactors= FALSE)
#st$start <- strptime(st$start,format="%d-%b-%Y %I:%M %p")
#st$stop  <- strptime(st$stop, format="%d-%b-%Y %I:%M %p")
print(st)
str(st)
# these are the models from the paper
load("mods.RData")
# these are functions to estimate features, load data, etclis
source("fuctions.for.models.60hz.R")
win.width <- 60     # width of "windows" to analyze in seconds aka EPOCH
#start of function to run in batch mode - similar to Macro in SAS
main <- function(id, file.1, start, stop, outfile) {
# convert strings to datetime format
start <- strptime(start,format="%d-%b-%Y %I:%M %p")
stop  <- strptime(stop, format="%d-%b-%Y %I:%M %p")
str(id)
str(file.1)
str(start)
str(stop)
str(outfile)
# get start time & start date from csv file header section
hertz = 60
head.data <- readLines(paste(file.1),n=10)
start.time <- head.data[3]
start.time <- (strsplit(start.time,split=" ")[[1]][3])
start.date <- head.data[4]
start.date <- (strsplit(start.date,split=" ")[[1]][3])
start.time <- as.POSIXlt(strptime(paste(start.date,start.time),"%m/%d/%Y %H:%M:%S"))
n.to.skip = as.numeric(difftime(start,start.time,units="secs")*hertz)
n.to.read = as.numeric(difftime(stop,start,units="secs")*hertz)
ag.data <- fread(file.1,skip=10+n.to.skip,sep=",",header=F,colClasses=c("numeric","numeric","numeric"),showProgress=FALSE,nrows=n.to.read)
n <- dim(ag.data)[1]
# data.frame adds a new column with times
ag.data <- data.frame(time = start + (0:(n-1))/hertz,ag.data)
n <- dim(ag.data)[1]
mins <- ceiling(n/(60*win.width))
# compute statistics (features)
ag.data$min <- rep(1:mins,each=win.width*60)[1:n]
ag.data$vm <- sqrt(ag.data$V1^2+ag.data$V2^2+ag.data$V3^2)
ag.data$v.ang <- 90*(asin(ag.data$V1/ag.data$vm)/(pi/2))
ag.data.sum <- data.frame(mean.vm=tapply(ag.data$vm,ag.data$min,mean,na.rm=T),
sd.vm=tapply(ag.data$vm,ag.data$min,sd,na.rm=T),
mean.ang=tapply(ag.data$v.ang,ag.data$min,mean,na.rm=T),
sd.ang=tapply(ag.data$v.ang,ag.data$min,sd,na.rm=T),
p625=tapply(ag.data$vm,ag.data$min,pow.625),
dfreq=tapply(ag.data$vm,ag.data$min,dom.freq),
ratio.df=tapply(ag.data$vm,ag.data$min,frac.pow.dom.freq))
# Next line can be slow... (there is a faster library, but I haven't implemented it yet.)
ag.data.sum$start.time <- as.POSIXlt(tapply(ag.data$time,ag.data$min,min,na.rm=T),origin="1970-01-01 00:00.00 UTC")
# apply the models (estimates are for each 15 second epoch)
# MET estimates by random forest
ag.data.sum$METs.rf <- predict(rf.met.model,newdata=ag.data.sum)
ag.data.sum$METs.rf[ag.data.sum$sd.vm==0] <- 1
# MET estimates by linear regression
ag.data.sum$METs.lm <- predict(lm.met.model,newdata=ag.data.sum)
ag.data.sum$METs.lm[ag.data.sum$sd.vm==0] <- 1
# MET level estimates (rf and tree)
ag.data.sum$MET.lev.rf <- predict(rf.met.level.model,newdata=ag.data.sum)
ag.data.sum$MET.lev.tr <- predict(tr.met.level.model,newdata=ag.data.sum,type="class")
# sedentary or not estimates (rf and tree)
ag.data.sum$sed.rf <- predict(rf.sed.model,newdata=ag.data.sum)
ag.data.sum$sed.tr <- predict(tr.sed.model,newdata=ag.data.sum,type="class")
# locomotion or not estimates (rf and tree)
ag.data.sum$loc.rf <- predict(rf.loc.model,newdata=ag.data.sum)
ag.data.sum$loc.tr <- predict(tr.loc.model,newdata=ag.data.sum,type="class")
# end of calculations
print("Done with calculations")
# export entire results vector (array) to .csv text file
write.csv(ag.data.sum,outfile, row.names=FALSE, quote=FALSE)
# end of main function
}
# call function, args are by position, call by value in R
mapply(main, st$Id, st$file.1, st$start, st$stop, st$outfile)
# end of program-- you should now see .out files for each day of accelerometer data for subject
# load needed R libraries  (make sure GGIR and zoo are installed)
library(GGIR)
library(zoo)
#g.shell.GGIR calls g.part1 - g.part4
#see manual for clarification on optional input arguments
##Usage  g.shell.GGIR(mode=c(1,2),datadir=c(),outputdir=c(),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
#manually change the folder to the correct subject number (not sure how to make loops in R)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/792"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
## ok so this is not working because for some reason I must not have write access to the hpc server. it worked when I output the data onto my desktop. so need to figure that out...
#run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/792"),outputdir=c("/Users/rachclark/Desktop"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
#example: run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Users/rachclark/Desktop/Accelerometer/775"),outputdir=c("/Volumes/vosslablss/Accelerometer/ActiGraph/Analysis/HildebrandAnalysis"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
install.packages("tree")
library(tree)
# batch mode for left/right wrist @60hz (originally created by Rick Paulos)
# first modified by Rachel Clark, 12/14/15
# incorporated into R notebook fall 2017
# comments start with # sign
# clear work space
rm(list=ls())
# load needed R libraries  (previously installed) -- make sure these are installed on your computer
library(tree)
library(randomForest)
library(data.table)
# Show installed libraries
ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)
# setwd sets working directory for your programs and data files
setwd("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data")
getwd()    # show wd to confirm
# read csv file with filenames, date(s) & time(s) on/off
st= read.csv("specs.csv", header=TRUE, sep=',', stringsAsFactors= FALSE)
#st$start <- strptime(st$start,format="%d-%b-%Y %I:%M %p")
#st$stop  <- strptime(st$stop, format="%d-%b-%Y %I:%M %p")
print(st)
str(st)
# these are the models from the paper
load("mods.RData")
# these are functions to estimate features, load data, etclis
source("fuctions.for.models.60hz.R")
win.width <- 60     # width of "windows" to analyze in seconds aka EPOCH
#start of function to run in batch mode - similar to Macro in SAS
main <- function(id, file.1, start, stop, outfile) {
# convert strings to datetime format
start <- strptime(start,format="%d-%b-%Y %I:%M %p")
stop  <- strptime(stop, format="%d-%b-%Y %I:%M %p")
str(id)
str(file.1)
str(start)
str(stop)
str(outfile)
# get start time & start date from csv file header section
hertz = 60
head.data <- readLines(paste(file.1),n=10)
start.time <- head.data[3]
start.time <- (strsplit(start.time,split=" ")[[1]][3])
start.date <- head.data[4]
start.date <- (strsplit(start.date,split=" ")[[1]][3])
start.time <- as.POSIXlt(strptime(paste(start.date,start.time),"%m/%d/%Y %H:%M:%S"))
n.to.skip = as.numeric(difftime(start,start.time,units="secs")*hertz)
n.to.read = as.numeric(difftime(stop,start,units="secs")*hertz)
ag.data <- fread(file.1,skip=10+n.to.skip,sep=",",header=F,colClasses=c("numeric","numeric","numeric"),showProgress=FALSE,nrows=n.to.read)
n <- dim(ag.data)[1]
# data.frame adds a new column with times
ag.data <- data.frame(time = start + (0:(n-1))/hertz,ag.data)
n <- dim(ag.data)[1]
mins <- ceiling(n/(60*win.width))
# compute statistics (features)
ag.data$min <- rep(1:mins,each=win.width*60)[1:n]
ag.data$vm <- sqrt(ag.data$V1^2+ag.data$V2^2+ag.data$V3^2)
ag.data$v.ang <- 90*(asin(ag.data$V1/ag.data$vm)/(pi/2))
ag.data.sum <- data.frame(mean.vm=tapply(ag.data$vm,ag.data$min,mean,na.rm=T),
sd.vm=tapply(ag.data$vm,ag.data$min,sd,na.rm=T),
mean.ang=tapply(ag.data$v.ang,ag.data$min,mean,na.rm=T),
sd.ang=tapply(ag.data$v.ang,ag.data$min,sd,na.rm=T),
p625=tapply(ag.data$vm,ag.data$min,pow.625),
dfreq=tapply(ag.data$vm,ag.data$min,dom.freq),
ratio.df=tapply(ag.data$vm,ag.data$min,frac.pow.dom.freq))
# Next line can be slow... (there is a faster library, but I haven't implemented it yet.)
ag.data.sum$start.time <- as.POSIXlt(tapply(ag.data$time,ag.data$min,min,na.rm=T),origin="1970-01-01 00:00.00 UTC")
# apply the models (estimates are for each 15 second epoch)
# MET estimates by random forest
ag.data.sum$METs.rf <- predict(rf.met.model,newdata=ag.data.sum)
ag.data.sum$METs.rf[ag.data.sum$sd.vm==0] <- 1
# MET estimates by linear regression
ag.data.sum$METs.lm <- predict(lm.met.model,newdata=ag.data.sum)
ag.data.sum$METs.lm[ag.data.sum$sd.vm==0] <- 1
# MET level estimates (rf and tree)
ag.data.sum$MET.lev.rf <- predict(rf.met.level.model,newdata=ag.data.sum)
ag.data.sum$MET.lev.tr <- predict(tr.met.level.model,newdata=ag.data.sum,type="class")
# sedentary or not estimates (rf and tree)
ag.data.sum$sed.rf <- predict(rf.sed.model,newdata=ag.data.sum)
ag.data.sum$sed.tr <- predict(tr.sed.model,newdata=ag.data.sum,type="class")
# locomotion or not estimates (rf and tree)
ag.data.sum$loc.rf <- predict(rf.loc.model,newdata=ag.data.sum)
ag.data.sum$loc.tr <- predict(tr.loc.model,newdata=ag.data.sum,type="class")
# end of calculations
print("Done with calculations")
# export entire results vector (array) to .csv text file
write.csv(ag.data.sum,outfile, row.names=FALSE, quote=FALSE)
# end of main function
}
# call function, args are by position, call by value in R
mapply(main, st$Id, st$file.1, st$start, st$stop, st$outfile)
# end of program-- you should now see .out files for each day of accelerometer data for subject
# load needed R libraries  (make sure GGIR and zoo are installed)
library(GGIR)
library(zoo)
# g.shell.GGIR calls g.part1 - g.part4
# see GGIR manual in 4-Analysis folder for clarification on optional input arguments
##Usage  g.shell.GGIR(mode=c(1,2),datadir=c(),outputdir=c(),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
# manually change the folder to the correct subject number
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/792"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
## Note: GGIR may stop working if the hpc server is too full and give a "read/write error".
rm(list=ls())
library(dplyr)
library(ggplot2)
rm(list=ls())
# load needed R libraries  (previously installed) -- make sure these are installed on your computer
library(tree)
install.packages("tree", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("randomForest", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("data.table", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
rm(list=ls())
# load needed R libraries  (previously installed) -- make sure these are installed on your computer
library(tree)
library(randomForest)
library(data.table)
# batch mode for left/right wrist @60hz (originally created by Rick Paulos)
# first modified by Rachel Clark, 12/14/15
# incorporated into R notebook fall 2017
# comments start with # sign
# clear work space
rm(list=ls())
# load needed R libraries  (previously installed) -- make sure these are installed on your computer
library(tree)
library(randomForest)
library(data.table)
# Show installed libraries
ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)
# setwd sets working directory for your programs and data files
setwd("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data")
getwd()    # show wd to confirm
# read csv file with filenames, date(s) & time(s) on/off
st= read.csv("specs.csv", header=TRUE, sep=',', stringsAsFactors= FALSE)
#st$start <- strptime(st$start,format="%d-%b-%Y %I:%M %p")
#st$stop  <- strptime(st$stop, format="%d-%b-%Y %I:%M %p")
print(st)
str(st)
# these are the models from the paper
load("mods.RData")
# these are functions to estimate features, load data, etclis
source("fuctions.for.models.60hz.R")
win.width <- 60     # width of "windows" to analyze in seconds aka EPOCH
#start of function to run in batch mode - similar to Macro in SAS
main <- function(id, file.1, start, stop, outfile) {
# convert strings to datetime format
start <- strptime(start,format="%d-%b-%Y %I:%M %p")
stop  <- strptime(stop, format="%d-%b-%Y %I:%M %p")
str(id)
str(file.1)
str(start)
str(stop)
str(outfile)
# get start time & start date from csv file header section
hertz = 60
head.data <- readLines(paste(file.1),n=10)
start.time <- head.data[3]
start.time <- (strsplit(start.time,split=" ")[[1]][3])
start.date <- head.data[4]
start.date <- (strsplit(start.date,split=" ")[[1]][3])
start.time <- as.POSIXlt(strptime(paste(start.date,start.time),"%m/%d/%Y %H:%M:%S"))
n.to.skip = as.numeric(difftime(start,start.time,units="secs")*hertz)
n.to.read = as.numeric(difftime(stop,start,units="secs")*hertz)
ag.data <- fread(file.1,skip=10+n.to.skip,sep=",",header=F,colClasses=c("numeric","numeric","numeric"),showProgress=FALSE,nrows=n.to.read)
n <- dim(ag.data)[1]
# data.frame adds a new column with times
ag.data <- data.frame(time = start + (0:(n-1))/hertz,ag.data)
n <- dim(ag.data)[1]
mins <- ceiling(n/(60*win.width))
# compute statistics (features)
ag.data$min <- rep(1:mins,each=win.width*60)[1:n]
ag.data$vm <- sqrt(ag.data$V1^2+ag.data$V2^2+ag.data$V3^2)
ag.data$v.ang <- 90*(asin(ag.data$V1/ag.data$vm)/(pi/2))
ag.data.sum <- data.frame(mean.vm=tapply(ag.data$vm,ag.data$min,mean,na.rm=T),
sd.vm=tapply(ag.data$vm,ag.data$min,sd,na.rm=T),
mean.ang=tapply(ag.data$v.ang,ag.data$min,mean,na.rm=T),
sd.ang=tapply(ag.data$v.ang,ag.data$min,sd,na.rm=T),
p625=tapply(ag.data$vm,ag.data$min,pow.625),
dfreq=tapply(ag.data$vm,ag.data$min,dom.freq),
ratio.df=tapply(ag.data$vm,ag.data$min,frac.pow.dom.freq))
# Next line can be slow... (there is a faster library, but I haven't implemented it yet.)
ag.data.sum$start.time <- as.POSIXlt(tapply(ag.data$time,ag.data$min,min,na.rm=T),origin="1970-01-01 00:00.00 UTC")
# apply the models (estimates are for each 15 second epoch)
# MET estimates by random forest
ag.data.sum$METs.rf <- predict(rf.met.model,newdata=ag.data.sum)
ag.data.sum$METs.rf[ag.data.sum$sd.vm==0] <- 1
# MET estimates by linear regression
ag.data.sum$METs.lm <- predict(lm.met.model,newdata=ag.data.sum)
ag.data.sum$METs.lm[ag.data.sum$sd.vm==0] <- 1
# MET level estimates (rf and tree)
ag.data.sum$MET.lev.rf <- predict(rf.met.level.model,newdata=ag.data.sum)
ag.data.sum$MET.lev.tr <- predict(tr.met.level.model,newdata=ag.data.sum,type="class")
# sedentary or not estimates (rf and tree)
ag.data.sum$sed.rf <- predict(rf.sed.model,newdata=ag.data.sum)
ag.data.sum$sed.tr <- predict(tr.sed.model,newdata=ag.data.sum,type="class")
# locomotion or not estimates (rf and tree)
ag.data.sum$loc.rf <- predict(rf.loc.model,newdata=ag.data.sum)
ag.data.sum$loc.tr <- predict(tr.loc.model,newdata=ag.data.sum,type="class")
# end of calculations
print("Done with calculations")
# export entire results vector (array) to .csv text file
write.csv(ag.data.sum,outfile, row.names=FALSE, quote=FALSE)
# end of main function
}
# call function, args are by position, call by value in R
mapply(main, st$Id, st$file.1, st$start, st$stop, st$outfile)
install.packages("GGIR", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("zoo", lib="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
# load needed R libraries  (make sure GGIR and zoo are installed)
library(GGIR)
library(zoo)
# g.shell.GGIR calls g.part1 - g.part4
# see GGIR manual in 4-Analysis folder for clarification on optional input arguments
##Usage  g.shell.GGIR(mode=c(1,2),datadir=c(),outputdir=c(),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
# manually change the folder to the correct subject number
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/801"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/157"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/800"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/803"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
run_shell=g.shell.GGIR(mode=c(1,2),datadir=c("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data/802"),outputdir=c("/Volumes/vosslabhpc/Projects/Accelerometer/5-Results"),studyname=c(),f0=1,f1=0,do.report=c(2),overwrite=FALSE,visualreport=FALSE)
## Note: GGIR may stop working if the hpc server is too full and give a "read/write error".
# batch mode for left/right wrist @60hz (originally created by Rick Paulos)
# first modified by Rachel Clark, 12/14/15
# incorporated into R notebook fall 2017
# comments start with # sign
# clear work space
rm(list=ls())
# load needed R libraries  (previously installed) -- make sure these are installed on your computer
library(tree)
library(randomForest)
library(data.table)
# Show installed libraries
ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)
# setwd sets working directory for your programs and data files
setwd("/Volumes/vosslabhpc/Projects/Accelerometer/3-Data")
getwd()    # show wd to confirm
# read csv file with filenames, date(s) & time(s) on/off
st= read.csv("specs.csv", header=TRUE, sep=',', stringsAsFactors= FALSE)
#st$start <- strptime(st$start,format="%d-%b-%Y %I:%M %p")
#st$stop  <- strptime(st$stop, format="%d-%b-%Y %I:%M %p")
print(st)
str(st)
# these are the models from the paper
load("mods.RData")
# these are functions to estimate features, load data, etclis
source("fuctions.for.models.60hz.R")
win.width <- 60     # width of "windows" to analyze in seconds aka EPOCH
#start of function to run in batch mode - similar to Macro in SAS
main <- function(id, file.1, start, stop, outfile) {
# convert strings to datetime format
start <- strptime(start,format="%d-%b-%Y %I:%M %p")
stop  <- strptime(stop, format="%d-%b-%Y %I:%M %p")
str(id)
str(file.1)
str(start)
str(stop)
str(outfile)
# get start time & start date from csv file header section
hertz = 60
head.data <- readLines(paste(file.1),n=10)
start.time <- head.data[3]
start.time <- (strsplit(start.time,split=" ")[[1]][3])
start.date <- head.data[4]
start.date <- (strsplit(start.date,split=" ")[[1]][3])
start.time <- as.POSIXlt(strptime(paste(start.date,start.time),"%m/%d/%Y %H:%M:%S"))
n.to.skip = as.numeric(difftime(start,start.time,units="secs")*hertz)
n.to.read = as.numeric(difftime(stop,start,units="secs")*hertz)
ag.data <- fread(file.1,skip=10+n.to.skip,sep=",",header=F,colClasses=c("numeric","numeric","numeric"),showProgress=FALSE,nrows=n.to.read)
n <- dim(ag.data)[1]
# data.frame adds a new column with times
ag.data <- data.frame(time = start + (0:(n-1))/hertz,ag.data)
n <- dim(ag.data)[1]
mins <- ceiling(n/(60*win.width))
# compute statistics (features)
ag.data$min <- rep(1:mins,each=win.width*60)[1:n]
ag.data$vm <- sqrt(ag.data$V1^2+ag.data$V2^2+ag.data$V3^2)
ag.data$v.ang <- 90*(asin(ag.data$V1/ag.data$vm)/(pi/2))
ag.data.sum <- data.frame(mean.vm=tapply(ag.data$vm,ag.data$min,mean,na.rm=T),
sd.vm=tapply(ag.data$vm,ag.data$min,sd,na.rm=T),
mean.ang=tapply(ag.data$v.ang,ag.data$min,mean,na.rm=T),
sd.ang=tapply(ag.data$v.ang,ag.data$min,sd,na.rm=T),
p625=tapply(ag.data$vm,ag.data$min,pow.625),
dfreq=tapply(ag.data$vm,ag.data$min,dom.freq),
ratio.df=tapply(ag.data$vm,ag.data$min,frac.pow.dom.freq))
# Next line can be slow... (there is a faster library, but I haven't implemented it yet.)
ag.data.sum$start.time <- as.POSIXlt(tapply(ag.data$time,ag.data$min,min,na.rm=T),origin="1970-01-01 00:00.00 UTC")
# apply the models (estimates are for each 15 second epoch)
# MET estimates by random forest
ag.data.sum$METs.rf <- predict(rf.met.model,newdata=ag.data.sum)
ag.data.sum$METs.rf[ag.data.sum$sd.vm==0] <- 1
# MET estimates by linear regression
ag.data.sum$METs.lm <- predict(lm.met.model,newdata=ag.data.sum)
ag.data.sum$METs.lm[ag.data.sum$sd.vm==0] <- 1
# MET level estimates (rf and tree)
ag.data.sum$MET.lev.rf <- predict(rf.met.level.model,newdata=ag.data.sum)
ag.data.sum$MET.lev.tr <- predict(tr.met.level.model,newdata=ag.data.sum,type="class")
# sedentary or not estimates (rf and tree)
ag.data.sum$sed.rf <- predict(rf.sed.model,newdata=ag.data.sum)
ag.data.sum$sed.tr <- predict(tr.sed.model,newdata=ag.data.sum,type="class")
# locomotion or not estimates (rf and tree)
ag.data.sum$loc.rf <- predict(rf.loc.model,newdata=ag.data.sum)
ag.data.sum$loc.tr <- predict(tr.loc.model,newdata=ag.data.sum,type="class")
# end of calculations
print("Done with calculations")
# export entire results vector (array) to .csv text file
write.csv(ag.data.sum,outfile, row.names=FALSE, quote=FALSE)
# end of main function
}
# call function, args are by position, call by value in R
mapply(main, st$Id, st$file.1, st$start, st$stop, st$outfile)
